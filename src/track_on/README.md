# Track-On: Transformer-based Online Point Tracking with Memory  

[**arXiv**](https://arxiv.org/abs/2501.18487) | [**Webpage**](https://kuis-ai.github.io/track_on)

This repository is the official implementation of the paper:

> **Track-On: Transformer-based Online Point Tracking with Memory**  
>
> [GÃ¶rkay Aydemir](https://gorkaydemir.github.io), Xiongyi Cai, [Weidi Xie](https://weidixie.github.io), [Fatma Guney](https://mysite.ku.edu.tr/fguney/)
>
> *International Conference on Learning Representations (ICLR), 2025*  
## Overview

**Track-On** is an efficient, **online point tracking** model that tracks points in a **frame-by-frame** manner using memory. It leverages a transformer-based architecture to maintain a compact yet effective memory of previously tracked points.

<p align="center">
  <img src="media/method_overview.png" alt="Track-On Overview" width="800" />
</p>

---

## Installation

### 1. Clone the repository
```bash
git clone https://github.com/gorkaydemir/track_on.git 
cd track_on
```

### 2. Set up the environment
```bash
conda create -n trackon python=3.8 -y
conda activate trackon
conda install pytorch==2.4.1 torchvision==0.19.1 torchaudio==2.4.1 pytorch-cuda=12.1 -c pytorch -c nvidia
pip install mmcv==2.2.0 -f https://download.openmmlab.com/mmcv/dist/cu121/torch2.4/index.html
pip install -r requirements.txt
```

### 3. Download Datasets  

To obtain the necessary datasets, follow the instructions provided in the [TAP-Vid repository](https://github.com/google-deepmind/tapnet/tree/main/tapnet/tapvid):  

- **Evaluation Datasets**:  
  - TAP-Vid Benchmark (DAVIS, RGB-Stacking, Kinetics)
  - Robo-TAP  

- **Training Dataset**:  
  - MOVi-F â€“ Refer to this [GitHub issue](https://github.com/facebookresearch/co-tracker/issues/8) for additional guidance.

---

## Quick Demo

Check out the [demo notebook](demo.ipynb) for a quick start with the model.

### Usage Options
Track-On provides two practical usage modes, both handling frames online but differing in input format:

#### 1. **Frame-by-frame input (for streaming videos)**
```python
from model.track_on_ff import TrackOnFF

model = TrackOnFF(args)
model.init_queries_and_memory(queries, first_frame)

while True:
    out = model.ff_forward(new_frame)
```

#### 2. **Video input (for benchmarking)**
```python
from model.track_on import TrackOn

model = TrackOn(args)
out = model.inference(video, queries)
```

---

## Evaluation

### 1. Download Pretrained Weights
Download the pre-trained checkpoint from [Hugging Face](https://huggingface.co/gaydemir/track_on/resolve/main/track_on_checkpoint.pt?download=true).

### 2. Run Evaluation
Given:
- `evaluation_dataset`: The dataset to evaluate on
- `tapvid_root`: Path to evaluation dataset
- `checkpoint_path`: Path to the downloaded checkpoint

Run the following command:
```bash
torchrun --master_port=12345 --nproc_per_node=1 main.py \
    --eval_dataset evaluation_dataset \
    --tapvid_root /path/to/eval/data \
    --checkpoint_path /path/to/checkpoint \
    --online_validation
```
This should reproduce the exact results reported in the paper when configured correctly.

---

## Training

### 1. Prepare datasets
- **Movi-f dataset**: Located at `/root/to/movi_f`
- **TAP-Vid evaluation dataset**:
  - Dataset name: `eval_dataset`
  - Path: `/root/to/tap_vid`
- **Training name**: `training_name`

### 2. Run Training
A multi-node training script is provided in [`train.sh`](scripts/train.sh). Default training arguments are set within the script.

---

## ğŸ“– Citation
If you find our work useful, please cite:
```bibtex
@InProceedings{Aydemir2025ICLR,
    author    = {Aydemir, G\"orkay and Cai, Xiongyi and Xie, Weidi and G\"uney, Fatma},
    title     = {{Track-On}: Transformer-based Online Point Tracking with Memory},
    booktitle = {The Thirteenth International Conference on Learning Representations},
    year      = {2025}
}
```

---

## Acknowledgments
This repository incorporates code from several public works, including [CoTracker](https://github.com/facebookresearch/co-tracker), [TAPNet](https://github.com/google-deepmind/tapnet), [DINOv2](https://github.com/facebookresearch/dinov2), [ViTAdapter](https://github.com/czczup/ViT-Adapter), and [SPINO](https://github.com/robot-learning-freiburg/SPINO). Special thanks to the authors of these projects for making their code available.

# H5è§†é¢‘æå–å·¥å…·

è¿™ä¸ªå·¥å…·ç”¨äºä»ç‰¹å®šæ ¼å¼çš„H5æ–‡ä»¶ä¸­æå–åŒæ‘„åƒå¤´è§†é¢‘æ•°æ®å¹¶è½¬æ¢ä¸ºMP4æ ¼å¼ã€‚

## ä¾èµ–é¡¹å®‰è£…

åœ¨è¿è¡Œè„šæœ¬å‰ï¼Œè¯·ç¡®ä¿å®‰è£…ä»¥ä¸‹ä¾èµ–ï¼š

```bash
pip install h5py numpy opencv-python tqdm
```

## ä½¿ç”¨æ–¹æ³•

1. å°†H5æ–‡ä»¶æ”¾åœ¨æŒ‡å®šç›®å½•ä¸‹
2. æ‰“å¼€`extract_videos.py`æ–‡ä»¶ï¼Œæ ¹æ®éœ€è¦ä¿®æ”¹ä»¥ä¸‹é…ç½®ï¼š
   - `h5_file_path`: H5æ–‡ä»¶çš„è·¯å¾„
   - `output_dir`: æå–çš„è§†é¢‘è¾“å‡ºç›®å½•
   - `fps`: è§†é¢‘å¸§ç‡ï¼ˆé»˜è®¤30fpsï¼‰
3. è¿è¡Œè„šæœ¬ï¼š

```bash
python extract_videos.py
```

4. è„šæœ¬ä¼šåœ¨æŒ‡å®šçš„è¾“å‡ºç›®å½•ä¸­ç”Ÿæˆä¸¤ä¸ªMP4è§†é¢‘æ–‡ä»¶ï¼Œåˆ†åˆ«å¯¹åº”ä¸¤ä¸ªæ‘„åƒå¤´ã€‚

## æ•°æ®ç»“æ„è¯´æ˜

è¯¥è„šæœ¬ä¸“é—¨é’ˆå¯¹åŒ…å«å¦‚ä¸‹ç»“æ„çš„H5æ–‡ä»¶ï¼š
- å¤šä¸ªåä¸º"step_X"çš„ç»„ï¼Œå…¶ä¸­Xæ˜¯å¸§åºå·
- æ¯ä¸ªstepç»„ä¸­åŒ…å«"obs/image"æ•°æ®é›†
- å›¾åƒæ•°æ®é›†å½¢çŠ¶ä¸º(1, 2, 3, 480, 640)ï¼Œåˆ†åˆ«ä»£è¡¨ï¼š
  - æ‰¹æ¬¡ç»´åº¦ (1)
  - æ‘„åƒå¤´æ•°é‡ (2)
  - é¢œè‰²é€šé“ (3) - RGB
  - å›¾åƒé«˜åº¦ (480)
  - å›¾åƒå®½åº¦ (640)

# çŠç‘šæ•°æ®track_onå¤„ç†æŒ‡å—

æœ¬æŒ‡å—è¯´æ˜å¦‚ä½•ä½¿ç”¨track_onå·¥å…·å¤„ç†çŠç‘šæ•°æ®é›†ã€‚

## å¤„ç†æµç¨‹

### 1. ä»æºæ•°æ®æå–è§†é¢‘
ä½¿ç”¨`convert_h5_to_mp4.py`ä»åŸå§‹H5æ–‡ä»¶ä¸­æå–å‡ºMP4è§†é¢‘å’Œç¬¬ä¸€å¸§å›¾ç‰‡(PNG)ï¼š

```bash
python3 convert_h5_to_mp4.py /home/nexus/workspaces/tracking/track_on/media/0509
```

### 2. æ ‡æ³¨å…³é”®ç‚¹
ä½¿ç”¨labelmeçš„create pointåŠŸèƒ½å¯¹æå–å‡ºçš„ç¬¬ä¸€å¸§å›¾ç‰‡è¿›è¡Œæ ‡æ³¨ï¼Œç”ŸæˆJSONæ ¼å¼çš„æ ‡æ³¨æ–‡ä»¶ã€‚

### 3. å…³é”®ç‚¹è¿½è¸ª
ä½¿ç”¨`track_on_videos.py`è¿›è¡Œå…³é”®ç‚¹è¿½è¸ªï¼š

```bash
python track_on_videos.py --input_dir /home/nexus/workspaces/tracking/dataset/0510/0509_cuihu/logs_pm --output_dir /home/nexus/workspaces/tracking/track_on/out/coral_dataset_0510_pm --checkpoint_path /home/nexus/workspaces/tracking/track_on/checkpoints/track_on_checkpoint.pt
```

### 4. æ£€æŸ¥è¿½è¸ªæ•ˆæœ
åœ¨è¾“å‡ºçš„æ¯æ¡æ•°æ®æ–‡ä»¶å¤¹ä¸­æ£€æŸ¥è¿½è¸ªæ•ˆæœï¼š
- å¦‚æœæ•ˆæœä¸å¥½ï¼Œåˆ™åˆ é™¤è¯¥æ•°æ®ï¼ˆå»ºè®®è®°å½•æœªé‡‡ç”¨çš„æ•°æ®ï¼‰
- å¦‚æœæ•ˆæœå¥½ï¼Œåˆ™å°†åŸå§‹H5æ–‡ä»¶ä¹Ÿå¤åˆ¶åˆ°è¯¥è¾“å‡ºæ–‡ä»¶å¤¹å†…

### 5. æ·»åŠ å…³é”®ç‚¹ä¿¡æ¯
æœ€åï¼Œè¿è¡Œ`add_keypoint_to_h5.py`å°†è¿½è¸ªå¥½çš„å…³é”®ç‚¹ä¿¡æ¯æ·»åŠ åˆ°H5æ–‡ä»¶ä¸­ï¼š

```bash
python add_keypoint_to_h5.py --input_dir media/coral_0509/20250809_1 --output_dir media/coral_0509/20250809_1_added_kp
```

